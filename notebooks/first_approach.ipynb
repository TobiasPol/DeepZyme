{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "masif_opts = {}\n",
    "# Default directories\n",
    "masif_opts[\"raw_pdb_dir\"] = \"../data/data_preparation/raw_pdbs\"\n",
    "masif_opts[\"pdb_chain_dir\"] = \"../data/data_preparation/01-benchmark_pdbs/\"\n",
    "masif_opts[\"ply_chain_dir\"] = \"../data/data_preparation/01-benchmark_surfaces/\"\n",
    "masif_opts[\"tmp_dir\"] = tempfile.gettempdir()\n",
    "masif_opts[\"ply_file_template\"] = masif_opts[\"ply_chain_dir\"] + \"/{}_{}.ply\"\n",
    "\n",
    "# Surface features\n",
    "masif_opts[\"use_hbond\"] = True\n",
    "masif_opts[\"use_hphob\"] = True\n",
    "masif_opts[\"use_apbs\"] = True\n",
    "masif_opts[\"compute_iface\"] = True\n",
    "# Mesh resolution. Everything gets very slow if it is lower than 1.0\n",
    "masif_opts[\"mesh_res\"] = 1.0\n",
    "masif_opts[\"feature_interpolation\"] = True\n",
    "\n",
    "\n",
    "# Coords params\n",
    "masif_opts[\"radius\"] = 12.0\n",
    "\n",
    "# Neural network patch application specific parameters.\n",
    "masif_opts[\"ppi_search\"] = {}\n",
    "masif_opts[\"ppi_search\"][\"training_list\"] = \"../data/lists/training.txt\"\n",
    "masif_opts[\"ppi_search\"][\"testing_list\"] = \"../data/lists/testing.txt\"\n",
    "masif_opts[\"ppi_search\"][\"max_shape_size\"] = 200\n",
    "masif_opts[\"ppi_search\"][\"max_distance\"] = 12.0  # Radius for the neural network.\n",
    "masif_opts[\"ppi_search\"][\n",
    "    \"masif_precomputation_dir\"\n",
    "] = \"../data/data_preparation/04b-precomputation_12A/precomputation/\"\n",
    "masif_opts[\"ppi_search\"][\"feat_mask\"] = [1.0] * 5\n",
    "masif_opts[\"ppi_search\"][\"max_sc_filt\"] = 1.0\n",
    "masif_opts[\"ppi_search\"][\"min_sc_filt\"] = 0.5\n",
    "masif_opts[\"ppi_search\"][\"pos_surf_accept_probability\"] = 1.0\n",
    "masif_opts[\"ppi_search\"][\"pos_interface_cutoff\"] = 1.0\n",
    "masif_opts[\"ppi_search\"][\"range_val_samples\"] = 0.9  # 0.9 to 1.0\n",
    "masif_opts[\"ppi_search\"][\"cache_dir\"] = \"nn_models/sc05/cache/\"\n",
    "masif_opts[\"ppi_search\"][\"model_dir\"] = \"nn_models/sc05/all_feat/model_data/\"\n",
    "masif_opts[\"ppi_search\"][\"desc_dir\"] = \"descriptors/sc05/all_feat/\"\n",
    "masif_opts[\"ppi_search\"][\"gif_descriptors_out\"] = \"gif_descriptors/\"\n",
    "# Parameters for shape complementarity calculations.\n",
    "masif_opts[\"ppi_search\"][\"sc_radius\"] = 12.0\n",
    "masif_opts[\"ppi_search\"][\"sc_interaction_cutoff\"] = 1.5\n",
    "masif_opts[\"ppi_search\"][\"sc_w\"] = 0.25\n",
    "\n",
    "# Neural network patch application specific parameters.\n",
    "masif_opts[\"site\"] = {}\n",
    "masif_opts[\"site\"][\"training_list\"] = \"lists/training.txt\"\n",
    "masif_opts[\"site\"][\"testing_list\"] = \"lists/testing.txt\"\n",
    "masif_opts[\"site\"][\"max_shape_size\"] = 100\n",
    "masif_opts[\"site\"][\"n_conv_layers\"] = 3\n",
    "masif_opts[\"site\"][\"max_distance\"] = 9.0  # Radius for the neural network.\n",
    "masif_opts[\"site\"][\n",
    "    \"masif_precomputation_dir\"\n",
    "] = \"data_preparation/04a-precomputation_9A/precomputation/\"\n",
    "masif_opts[\"site\"][\"range_val_samples\"] = 0.9  # 0.9 to 1.0\n",
    "masif_opts[\"site\"][\"model_dir\"] = \"nn_models/all_feat_3l/model_data/\"\n",
    "masif_opts[\"site\"][\"out_pred_dir\"] = \"output/all_feat_3l/pred_data/\"\n",
    "masif_opts[\"site\"][\"out_surf_dir\"] = \"output/all_feat_3l/pred_surfaces/\"\n",
    "masif_opts[\"site\"][\"feat_mask\"] = [1.0] * 5\n",
    "\n",
    "# Neural network ligand application specific parameters.\n",
    "masif_opts[\"ligand\"] = {}\n",
    "masif_opts[\"ligand\"][\"assembly_dir\"] = \"data_preparation/00b-pdbs_assembly\"\n",
    "masif_opts[\"ligand\"][\"ligand_coords_dir\"] = \"data_preparation/00c-ligand_coords\"\n",
    "masif_opts[\"ligand\"][\n",
    "    \"masif_precomputation_dir\"\n",
    "] = \"data_preparation/04a-precomputation_12A/precomputation/\"\n",
    "masif_opts[\"ligand\"][\"max_shape_size\"] = 200\n",
    "masif_opts[\"ligand\"][\"feat_mask\"] = [1.0] * 5\n",
    "masif_opts[\"ligand\"][\"train_fract\"] = 0.9 * 0.8\n",
    "masif_opts[\"ligand\"][\"val_fract\"] = 0.1 * 0.8\n",
    "masif_opts[\"ligand\"][\"test_fract\"] = 0.2\n",
    "masif_opts[\"ligand\"][\"tfrecords_dir\"] = \"data_preparation/tfrecords\"\n",
    "masif_opts[\"ligand\"][\"max_distance\"] = 12.0\n",
    "masif_opts[\"ligand\"][\"n_classes\"] = 7\n",
    "masif_opts[\"ligand\"][\"feat_mask\"] = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "masif_opts[\"ligand\"][\"costfun\"] = \"dprime\"\n",
    "masif_opts[\"ligand\"][\"model_dir\"] = \"nn_models/all_feat/\"\n",
    "masif_opts[\"ligand\"][\"test_set_out_dir\"] = \"test_set_predictions/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDB Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Bio.PDB import PDBList\n",
    "\n",
    "def pdb_download(tsv_file_path, pdb_directory):\n",
    "    \"\"\"\n",
    "    Lädt PDB-Dateien für die in der TSV-Datei angegebenen PDB-IDs herunter.\n",
    "\n",
    "    :param tsv_file_path: Pfad zur TSV-Datei mit den PDB-IDs\n",
    "    :param pdb_directory: Verzeichnis, in dem die PDB-Dateien gespeichert werden sollen\n",
    "    \"\"\"\n",
    "\n",
    "    # Lese die TSV-Datei und extrahiere die PDB-IDs\n",
    "    data = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "    # Nimm nur die ersten 10 Einträge (zum Testen)\n",
    "    data = data.head(10)\n",
    "    # Nimm nur eindeute PDB-IDs und entferne alle unnötigen Zeichen\n",
    "    data['PDB'] = data['PDB'].apply(lambda x: x.split(';')[0].strip().split(',')[0].strip())\n",
    "    pdb_ids = data['PDB'].dropna().unique()\n",
    "\n",
    "    # Erstellen eines Verzeichnisses für die heruntergeladenen PDB-Dateien\n",
    "    pdb_directory = '../data/pdb_files'\n",
    "    os.makedirs(pdb_directory, exist_ok=True)\n",
    "\n",
    "    # PDB-Downloader initialisieren\n",
    "    pdbl = PDBList()\n",
    "\n",
    "    # Durchlaufe alle PDB-IDs und lade die Dateien herunter\n",
    "    for pdb_id in pdb_ids:\n",
    "        print(f\"Downloading PDB file for {pdb_id}...\")\n",
    "        try:\n",
    "            # Versuche die PDB-Datei herunterzuladen\n",
    "            filename = pdbl.retrieve_pdb_file(pdb_id, pdir=pdb_directory, file_format='pdb')\n",
    "            correct_filename = os.path.join(pdb_directory, f\"{pdb_id.lower()}.pdb\")\n",
    "            if not filename.endswith('.pdb'):\n",
    "                os.rename(filename, correct_filename)\n",
    "            print(f\"PDB file saved as: {correct_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_id}: {e}\")\n",
    "\n",
    "    print(\"All PDB files downloaded.\")\n",
    "    print(f\"Saved in directory: {pdb_directory}\")\n",
    "\n",
    "    return pdb_directory\n",
    "\n",
    "pdb_download(\"../data/data_preparation/uniprot/raw_enzyme_list.tsv\", \"../data/data_preparation/raw_pdbs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from SBILib.structure import PDB\n",
    "\n",
    "\n",
    "def generate_assembly(pdb_id, raw_pdb_dir, assembly_dir):\n",
    "    \"\"\"\n",
    "    Lädt eine PDB-Datei herunter und generiert die biologische Assemblierung.\n",
    "\n",
    "    :param pdb_id: PDB-ID\n",
    "    :param raw_pdb_dir: Verzeichnis, in dem die PDB-Dateien gespeichert werden sollen\n",
    "    :param assembly_dir: Verzeichnis, in dem die biologischen Assemblierungen gespeichert werden sollen\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    ligands = [\"ADP\", \"COA\", \"FAD\", \"HEM\", \"NAD\", \"NAP\", \"SAM\"]\n",
    "\n",
    "    if not os.path.exists(assembly_dir):\n",
    "        os.mkdir(assembly_dir)\n",
    "\n",
    "    print(pdb_id)\n",
    "\n",
    "    def assemble(pdb_id):\n",
    "        # Reads and builds the biological assembly of a structure\n",
    "        print(\"Building assembly for\", pdb_id)\n",
    "        struct = PDB(os.path.join(raw_pdb_dir, pdb_id + \".pdb\"))\n",
    "        try:\n",
    "            struct_assembly = struct.apply_biomolecule_matrices()[0]\n",
    "        except:\n",
    "            return 0\n",
    "        struct_assembly.write(\n",
    "            os.path.join(assembly_dir, \"{}.pdb\".format(pdb_id))\n",
    "        )\n",
    "        return 1\n",
    "\n",
    "\n",
    "    in_fields = sys.argv[1].split(\"_\")\n",
    "    #pdb_id = in_fields[0]\n",
    "\n",
    "    res = assemble(pdb_id)\n",
    "    if res:\n",
    "        print(\"Building assembly was successfull for {}\".format(pdb_id))\n",
    "    else:\n",
    "        print(\"Building assembly was not successfull for {}\".format(pdb_id))\n",
    "\n",
    "# Generate assemblies for all PDB files\n",
    "raw_pdb_dir = \"../data/data_preparation/raw_pdbs\"\n",
    "assembly_dir = \"../data/data_preparation/assembly_dir\"\n",
    "pdb_files = os.listdir(raw_pdb_dir)\n",
    "pdb_ids = [f.split(\".\")[0] for f in pdb_files]\n",
    "for pdb_id in pdb_ids:\n",
    "    generate_assembly(pdb_id, raw_pdb_dir, assembly_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Ligand Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from SBILib.structure import PDB\n",
    "\n",
    "def save_ligand_coords(pdb_id, ligand_coords_dir, assembly_dir):\n",
    "\n",
    "    if not os.path.exists(ligand_coords_dir):\n",
    "        os.mkdir(ligand_coords_dir)\n",
    "\n",
    "    # Ligands of interest\n",
    "    ligands = [\"ADP\", \"COA\", \"FAD\", \"HEM\", \"NAD\", \"NAP\", \"SAM\"]\n",
    "\n",
    "    structure_ligands_type = []\n",
    "    structure_ligands_coords = []\n",
    "\n",
    "    try:\n",
    "        structure = PDB(os.path.join(assembly_dir, pdb_id + \".pdb\"))\n",
    "    except:\n",
    "        print(\"Problem with opening structure\", pdb_id)\n",
    "    for chain in structure.chains:\n",
    "        for het in chain.heteroatoms:\n",
    "            # Check all ligands in structure and save coordinates if they are of interest\n",
    "            if het.type in ligands:\n",
    "                structure_ligands_type.append(het.type)\n",
    "                structure_ligands_coords.append(het.all_coordinates)\n",
    "\n",
    "    np.save(\n",
    "        os.path.join(\n",
    "            ligand_coords_dir, \"{}_ligand_types.npy\".format(pdb_id)\n",
    "        ),\n",
    "        structure_ligands_type,\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(\n",
    "            ligand_coords_dir, \"{}_ligand_coords.npy\".format(pdb_id)\n",
    "        ),\n",
    "        structure_ligands_coords,\n",
    "    )\n",
    "\n",
    "# Save ligand coordinates for all PDB files\n",
    "ligand_coords_dir = \"../data/data_preparation/ligand_coords\"\n",
    "assembly_dir = \"../data/data_preparation/assembly_dir\"\n",
    "pdb_files = os.listdir(assembly_dir)\n",
    "pdb_ids = [f.split(\".\")[0] for f in pdb_files]\n",
    "for pdb_id in pdb_ids:\n",
    "    save_ligand_coords(pdb_id, ligand_coords_dir, assembly_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Triangulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/tobias.polley/Repositories/DeepZyme/masif_seed/masif/source/default_config/global_vars.py\u001b[0m(14)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     12 \u001b[0;31m\u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     13 \u001b[0;31m  \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 14 \u001b[0;31m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR: MSMS_BIN not set. Variable should point to MSMS program.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m  \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "import os\n",
    "import Bio\n",
    "import shutil\n",
    "from Bio.PDB import * \n",
    "import sys\n",
    "import importlib\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "#import pymesh\n",
    "\n",
    "# Local includes\n",
    "import sys  \n",
    "sys.path.insert(1, '../masif_seed/masif/source/triangulation')\n",
    "sys.path.insert(1, '../masif_seed/masif/source/input_output')\n",
    "import computeMSMS\n",
    "import fixmesh\n",
    "import extractPDB\n",
    "import save_ply\n",
    "import read_ply\n",
    "import protonate\n",
    "import computeHydrophobicity\n",
    "import computeCharges, assignChargesToNewMesh\n",
    "import computeAPBS\n",
    "import compute_normal\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "def extract_and_triangulate(masif_opts, pdb_id):\n",
    "\n",
    "    # Save the chains as separate files.\n",
    "    in_fields = sys.argv[1].split(\"_\")\n",
    "    pdb_id = in_fields[0]\n",
    "    chain_ids1 = in_fields[1]\n",
    "\n",
    "    if (len(sys.argv)>2) and (sys.argv[2]=='masif_ligand'):\n",
    "        pdb_filename = os.path.join(masif_opts[\"ligand\"][\"assembly_dir\"],pdb_id+\".pdb\")\n",
    "    else:\n",
    "        pdb_filename = masif_opts['raw_pdb_dir']+pdb_id+\".pdb\"\n",
    "    tmp_dir= masif_opts['tmp_dir']\n",
    "    protonated_file = tmp_dir+\"/\"+pdb_id+\".pdb\"\n",
    "    protonate(pdb_filename, protonated_file)\n",
    "    pdb_filename = protonated_file\n",
    "\n",
    "    # Extract chains of interest.\n",
    "    out_filename1 = tmp_dir+\"/\"+pdb_id+\"_\"+chain_ids1\n",
    "    extractPDB(pdb_filename, out_filename1+\".pdb\", chain_ids1)\n",
    "\n",
    "    # Compute MSMS of surface w/hydrogens, \n",
    "    try:\n",
    "        vertices1, faces1, normals1, names1, areas1 = computeMSMS(out_filename1+\".pdb\",\\\n",
    "            protonate=True)\n",
    "    except:\n",
    "        set_trace()\n",
    "\n",
    "    # Compute \"charged\" vertices\n",
    "    if masif_opts['use_hbond']:\n",
    "        vertex_hbond = computeCharges(out_filename1, vertices1, names1)\n",
    "\n",
    "    # For each surface residue, assign the hydrophobicity of its amino acid. \n",
    "    if masif_opts['use_hphob']:\n",
    "        vertex_hphobicity = computeHydrophobicity(names1)\n",
    "\n",
    "    # If protonate = false, recompute MSMS of surface, but without hydrogens (set radius of hydrogens to 0).\n",
    "    vertices2 = vertices1\n",
    "    faces2 = faces1\n",
    "\n",
    "    # Fix the mesh.\n",
    "    mesh = pymesh.form_mesh(vertices2, faces2)\n",
    "    regular_mesh = fix_mesh(mesh, masif_opts['mesh_res'])\n",
    "\n",
    "    # Compute the normals\n",
    "    vertex_normal = compute_normal(regular_mesh.vertices, regular_mesh.faces)\n",
    "    # Assign charges on new vertices based on charges of old vertices (nearest\n",
    "    # neighbor)\n",
    "\n",
    "    if masif_opts['use_hbond']:\n",
    "        vertex_hbond = assignChargesToNewMesh(regular_mesh.vertices, vertices1,\\\n",
    "            vertex_hbond, masif_opts)\n",
    "\n",
    "    if masif_opts['use_hphob']:\n",
    "        vertex_hphobicity = assignChargesToNewMesh(regular_mesh.vertices, vertices1,\\\n",
    "            vertex_hphobicity, masif_opts)\n",
    "\n",
    "    if masif_opts['use_apbs']:\n",
    "        vertex_charges = computeAPBS(regular_mesh.vertices, out_filename1+\".pdb\", out_filename1)\n",
    "\n",
    "    iface = np.zeros(len(regular_mesh.vertices))\n",
    "    if 'compute_iface' in masif_opts and masif_opts['compute_iface']:\n",
    "        # Compute the surface of the entire complex and from that compute the interface.\n",
    "        v3, f3, _, _, _ = computeMSMS(pdb_filename,\\\n",
    "            protonate=True)\n",
    "        # Regularize the mesh\n",
    "        mesh = pymesh.form_mesh(v3, f3)\n",
    "        # I believe It is not necessary to regularize the full mesh. This can speed up things by a lot.\n",
    "        full_regular_mesh = mesh\n",
    "        # Find the vertices that are in the iface.\n",
    "        v3 = full_regular_mesh.vertices\n",
    "        # Find the distance between every vertex in regular_mesh.vertices and those in the full complex.\n",
    "        kdt = KDTree(v3)\n",
    "        d, r = kdt.query(regular_mesh.vertices)\n",
    "        d = np.square(d) # Square d, because this is how it was in the pyflann version.\n",
    "        assert(len(d) == len(regular_mesh.vertices))\n",
    "        iface_v = np.where(d >= 2.0)[0]\n",
    "        iface[iface_v] = 1.0\n",
    "        # Convert to ply and save.\n",
    "        save_ply(out_filename1+\".ply\", regular_mesh.vertices,\\\n",
    "                            regular_mesh.faces, normals=vertex_normal, charges=vertex_charges,\\\n",
    "                            normalize_charges=True, hbond=vertex_hbond, hphob=vertex_hphobicity,\\\n",
    "                            iface=iface)\n",
    "\n",
    "    else:\n",
    "        # Convert to ply and save.\n",
    "        save_ply(out_filename1+\".ply\", regular_mesh.vertices,\\\n",
    "                            regular_mesh.faces, normals=vertex_normal, charges=vertex_charges,\\\n",
    "                            normalize_charges=True, hbond=vertex_hbond, hphob=vertex_hphobicity)\n",
    "    if not os.path.exists(masif_opts['ply_chain_dir']):\n",
    "        os.makedirs(masif_opts['ply_chain_dir'])\n",
    "    if not os.path.exists(masif_opts['pdb_chain_dir']):\n",
    "        os.makedirs(masif_opts['pdb_chain_dir'])\n",
    "    shutil.copy(out_filename1+'.ply', masif_opts['ply_chain_dir']) \n",
    "    shutil.copy(out_filename1+'.pdb', masif_opts['pdb_chain_dir'])\n",
    "\n",
    "# Extract and triangulate all PDB files\n",
    "pdb_files = os.listdir(masif_opts['raw_pdb_dir'])\n",
    "pdb_ids = [f.split(\".\")[0] for f in pdb_files]\n",
    "for pdb_id in pdb_ids:\n",
    "    extract_and_triangulate(masif_opts, pdb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio.PDB import PDBParser, Select\n",
    "\n",
    "class NonHeteroSelect(Select):\n",
    "    \"\"\" Klasse, um heterogene Atome aus der Struktur zu entfernen. \"\"\"\n",
    "    def accept_residue(self, residue):\n",
    "        return residue.id[0] == ' '\n",
    "\n",
    "# Verzeichnis mit den PDB-Dateien\n",
    "pdb_directory = '../data/pdb_files'\n",
    "processed_directory = '../data/processed_pdb_files'\n",
    "os.makedirs(processed_directory, exist_ok=True)\n",
    "\n",
    "# PDB-Parser initialisieren\n",
    "parser = PDBParser()\n",
    "\n",
    "# Liste der PDB-Dateien\n",
    "pdb_files = [f for f in os.listdir(pdb_directory) if f.endswith('.pdb')]\n",
    "print(f\"Found {len(pdb_files)} PDB files in {pdb_directory}\")\n",
    "\n",
    "# Durchlaufe alle PDB-Dateien\n",
    "for pdb_file in pdb_files:\n",
    "    structure_id = pdb_file.split('.')[0]\n",
    "    filename = os.path.join(pdb_directory, pdb_file)\n",
    "    structure = parser.get_structure(structure_id, filename)\n",
    "\n",
    "    # Entferne heterogene Atome\n",
    "    from Bio.PDB import PDBIO\n",
    "    io = PDBIO()\n",
    "    io.set_structure(structure)\n",
    "    processed_filename = os.path.join(processed_directory, pdb_file)\n",
    "    io.save(processed_filename, NonHeteroSelect())\n",
    "\n",
    "    print(f\"Processed {pdb_file} and saved to {processed_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
